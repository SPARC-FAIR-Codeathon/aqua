{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e8f093",
   "metadata": {},
   "source": [
    "# autocomplete data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data test for autocomplete\n",
    "import json\n",
    "\n",
    "with open('words_autocomplete1.json', 'r') as fp:\n",
    "    w_autocomplete = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomnly select 200 words from 465142 1 gram and 2 gram in datasets and NIFS Ontology\n",
    "# - select word with length between 3 and 15\n",
    "# - word do not contain number\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "words = list(w_autocomplete.keys())\n",
    "testwords = []\n",
    "for i in range(200):\n",
    "    while True:\n",
    "        pos = random.randint(0, len(w_autocomplete)-1)\n",
    "        word = words[pos]\n",
    "        if '/' not in word and len(word)>2 and len(word) < 15 and not bool(re.search(r'\\d', word)) and len(word.split())==1:\n",
    "            testwords += [word]\n",
    "            break\n",
    "\n",
    "# save data test to file\n",
    "with open('test_autocomplete_pure.json', 'w') as fp:\n",
    "    json.dump(testwords, fp)\n",
    "    \n",
    "# prepare data for 1 typo\n",
    "test_1typo = []\n",
    "for word in testwords:\n",
    "    pos = random.randint(1, len(word)-1) \n",
    "    test_1typo += [word[:pos]+'*'+word[pos+1:]]\n",
    "\n",
    "# save data test typo to file\n",
    "with open('test_autocomplete_typo.json', 'w') as fp:\n",
    "    json.dump(test_1typo, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9b4fd",
   "metadata": {},
   "source": [
    "# query data test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98649b0",
   "metadata": {},
   "source": [
    "We generated test collections by randomly select keywords and authors related to the SPARC data sets, including keywords and authors. Hence,  test collection is a pair of query and a list of the corresponding data sets. Then, we differentiated the test collections based on the number of terms in the query and mimicking typos by performing insertion, deletion, replacement, and spaces removal. In total,there are 31 test collections consisting 50 pairs of query and a list of datasets.\n",
    "\n",
    "Here are the type of the test collection number of terms in the query we used:\n",
    "- keyword\n",
    "    - 1 term query\n",
    "        - no typo\n",
    "        - 1 deletion\n",
    "        - 1 insertion\n",
    "        - 1 replacement\n",
    "    - 2 terms query\n",
    "        - no typo\n",
    "        - 1 deletion\n",
    "        - 1 insertion\n",
    "        - 1 replacement\n",
    "        - no space\n",
    "        - no space with 1 typo\n",
    "        - no space with 2 typos\n",
    "        - no space with 3 typos\n",
    "        - 3 typos\n",
    "    - 3 terms query\n",
    "        - no typo\n",
    "        - 1 deletion\n",
    "        - 1 insertion\n",
    "        - 1 replacement\n",
    "        - no space\n",
    "        - no space with 1 typo\n",
    "        - no space with 2 typos\n",
    "        - no space with 3 typos\n",
    "        - 3 typos\n",
    "- author\n",
    "    - 1 term query\n",
    "        - no typo\n",
    "        - 1 deletion\n",
    "        - 1 insertion\n",
    "        - 1 replacement\n",
    "    - 2 terms query\n",
    "        - no typo\n",
    "        - 1 deletion\n",
    "        - 1 insertion\n",
    "        - 1 replacement\n",
    "        - no space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c057f239",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# a function to retrieve information from json using path\n",
    "def __find(element, JSON):\n",
    "    \"\"\"\n",
    "    To find the content in elasticsearch's hits based on path in element.\n",
    "    Arguments:\n",
    "        - element: a path to the content, e.g. _source.item.keywords.keyword\n",
    "        - JSON: a dictionary as a result of elasticsearch query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paths = element.split(\".\")\n",
    "        data = JSON\n",
    "        for count, p in enumerate(paths):\n",
    "            if isinstance(data[p], dict):\n",
    "                data = data[p]\n",
    "            elif isinstance(data[p], list):\n",
    "                data = [__find(element[element.find(p)+len(p)+1:], lst) for lst in data[p]]\n",
    "                break\n",
    "            elif len(paths)-1 == count:\n",
    "                return data[p]\n",
    "        if [] in data: data.remove([])\n",
    "        return data\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "# open a file containing the metadata of the datasets\n",
    "import json\n",
    "with open('sparc_datasets.json', 'r') as fp:\n",
    "    sparc_meta = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "4c48595d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# organise metadata\n",
    "term2dataset = {} # term to list of dataset (the location in hit list is used to determine id, not the dataset identifier)\n",
    "keyword2dataset = {} # keyword to list of dataset\n",
    "keywords = {} # keyword based on length {1:{...}, 2:{...}, ...}\n",
    "\n",
    "author2dataset = {} # author to list of dataset\n",
    "sname2dataset = {} # name to list of dataset\n",
    "\n",
    "for hit in enumerate(sparc_meta['hits']['hits']):\n",
    "    try:\n",
    "        idx = hit['_source']['pennsieve']['identifier']\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for keyword in __find('_source.item.keywords.keyword', hit):\n",
    "        keyword = keyword.lower()\n",
    "        length = len(keyword.split())\n",
    "        if length not in keywords: keywords[length] = []\n",
    "        keywords[length] += [keyword]\n",
    "        if keyword not in keyword2dataset: keyword2dataset[keyword] = []\n",
    "        keyword2dataset[keyword] += [idx]\n",
    "        for term in keyword.split():\n",
    "            if term not in term2dataset: term2dataset[term] = []\n",
    "            term2dataset[term] += [idx]\n",
    "    for length in keywords:\n",
    "        keywords[length] = list(set(keywords[length]))\n",
    "            \n",
    "    firsts = __find('_source.contributors.first.name', hit)\n",
    "    lasts = __find('_source.contributors.last.name', hit)\n",
    "    for name in [first+' '+last for first, last in (zip(firsts, lasts))]:\n",
    "        name = name.lower()\n",
    "        if name not in author2dataset: author2dataset[name] = []\n",
    "        author2dataset[name] += [idx]\n",
    "    for first in firsts:\n",
    "        first = first.lower()\n",
    "        if first not in sname2dataset: sname2dataset[first] = []\n",
    "        sname2dataset[first] += [idx]\n",
    "    for last in lasts:\n",
    "        last = last.lower()\n",
    "        if last not in sname2dataset: sname2dataset[last] = []\n",
    "        sname2dataset[last] += [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "0cf7a53a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# a function to generate datatest\n",
    "# parameters: - numOfTerm (int) : the number of term in a query\n",
    "#             - numOfData (int) : the number of generated test queries\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def generateDataTest(numOfTerm, numOfData):\n",
    "    dataTest = {}\n",
    "    if numOfData < len(keywords[numOfTerm]):\n",
    "        kws = copy.deepcopy(keywords[numOfTerm])\n",
    "        while len(dataTest) < numOfData:\n",
    "            keyword = random.choice(kws)\n",
    "            dataTest[keyword] = list(set(keyword2dataset[keyword]))\n",
    "            kws.remove(keyword)\n",
    "    else:\n",
    "        for keyword in keywords[numOfTerm]:\n",
    "            dataTest[keyword] = list(set(keyword2dataset[keyword]))\n",
    "        \n",
    "        kws = list(term2dataset.keys())\n",
    "        while len(dataTest) < numOfData:\n",
    "            if numOfTerm in [1, 2]:\n",
    "                keyword = ''\n",
    "                idxs = []\n",
    "                for i in range(numOfTerm):\n",
    "                    term = ''\n",
    "                    while len(term)<=3:\n",
    "                        term = random.choice(kws)\n",
    "                    keyword += term + ' '\n",
    "                    idxs += term2dataset[term]\n",
    "                keyword = keyword.strip()\n",
    "                if keyword not in dataTest:\n",
    "                    dataTest[keyword] = list(set(idxs))\n",
    "            elif numOfTerm == 3:\n",
    "                keyword = ''\n",
    "                term1 = random.choice(list(keywords[2]))\n",
    "                term2 = random.choice(kws)\n",
    "                keyword = term1 + ' ' + term2\n",
    "                if keyword not in dataTest:\n",
    "                    dataTest[keyword] = list(set(keyword2dataset[term1]))\n",
    "    return dataTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "be744581",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create 1 terms dataset\n",
    "query_1_terms = generateDataTest(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b3412bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 terms dataset\n",
    "query_2_terms = generateDataTest(2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "78d6701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 terms dataset\n",
    "query_3_terms = generateDataTest(3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f3fe647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import string\n",
    "# dataset 1 term 1 deletion\n",
    "def get_1_deletion(oldData):\n",
    "    newData = copy.deepcopy(oldData)\n",
    "    for query in list(newData.keys()):\n",
    "        pos = random.randint(0, len(query)-1)\n",
    "        newQuery = query[:pos] + query[pos+1:]\n",
    "        newData[newQuery] = newData[query]\n",
    "        del newData[query]\n",
    "    return newData\n",
    "    \n",
    "# dataset 1 term 1 insertion\n",
    "def get_1_insertion(oldData):\n",
    "    newData= copy.deepcopy(oldData)\n",
    "    for query in list(newData.keys()):\n",
    "        pos = random.randint(0, len(query)-1)\n",
    "        char = random.choice(string.ascii_lowercase)\n",
    "        newQuery = query[:pos] + char + query[pos:]\n",
    "        newData[newQuery] = newData[query]\n",
    "        del newData[query]\n",
    "    return newData\n",
    "    \n",
    "# dataset 1 term 1 replacement\n",
    "def get_1_replacement(oldData):\n",
    "    newData= copy.deepcopy(oldData)\n",
    "    for query in list(newData.keys()):\n",
    "        pos = random.randint(0, len(query)-1)\n",
    "        char = random.choice(string.ascii_lowercase)\n",
    "        newQuery = query[:pos] + char + query[pos+1:]\n",
    "        newData[newQuery] = newData[query]\n",
    "        del newData[query]\n",
    "    return newData\n",
    "\n",
    "# remove space\n",
    "def removeSpace(oldData):\n",
    "    newData = copy.deepcopy(oldData)\n",
    "    for query in list(newData.keys()):\n",
    "        newQuery = query.replace(' ','')\n",
    "        newData[newQuery] = newData[query]\n",
    "        del newData[query]\n",
    "    return newData\n",
    "\n",
    "# 1 random distance\n",
    "def get_1_distance(oldData):\n",
    "    newData= copy.deepcopy(oldData)\n",
    "    for query in list(newData.keys()):\n",
    "        pos = random.randint(0, len(query)-1)\n",
    "        char = random.choice(string.ascii_lowercase)\n",
    "        distType = random.randint(0,2)\n",
    "        if distType == 0:\n",
    "            newQuery = query[:pos] + query[pos+1:]\n",
    "        elif distType == 1:\n",
    "            newQuery = query[:pos] + char + query[pos:]\n",
    "        else:\n",
    "            newQuery = query[:pos] + char + query[pos+1:]\n",
    "        newData[newQuery] = newData[query]\n",
    "        del newData[query]\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "582c1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest = {\n",
    "    1:{\n",
    "        '0 typo': query_1_terms,\n",
    "        '1 del': get_1_deletion(query_1_terms),\n",
    "        '1 insert': get_1_insertion(query_1_terms),\n",
    "        '1 replace': get_1_replacement(query_1_terms),\n",
    "    },\n",
    "    2:{\n",
    "        '0 typo': query_2_terms,\n",
    "        '1 del': get_1_deletion(query_2_terms),\n",
    "        '1 insert': get_1_insertion(query_2_terms),\n",
    "        '1 replace': get_1_replacement(query_2_terms),\n",
    "        'no space': removeSpace(query_2_terms),\n",
    "        'no space 1 typo': get_1_distance(removeSpace(query_2_terms)),\n",
    "        'no space 2 typo': get_1_distance(get_1_distance(removeSpace(query_2_terms))),\n",
    "        'no space 3 typo': get_1_deletion(get_1_insertion(get_1_replacement(removeSpace(query_2_terms)))),\n",
    "        '3 typo': get_1_deletion(get_1_insertion(get_1_replacement(query_2_terms))),\n",
    "    },\n",
    "    3:{\n",
    "        '0 typo': query_3_terms,\n",
    "        '1 del': get_1_deletion(query_3_terms),\n",
    "        '1 insert': get_1_insertion(query_3_terms),\n",
    "        '1 replace': get_1_replacement(query_3_terms),\n",
    "        'no space': removeSpace(query_3_terms),\n",
    "        'no space 1 typo': get_1_distance(removeSpace(query_3_terms)),\n",
    "        'no space 2 typo': get_1_distance(get_1_distance(removeSpace(query_3_terms))),\n",
    "        'no space 3 typo': get_1_deletion(get_1_insertion(get_1_replacement(removeSpace(query_3_terms)))),\n",
    "        '3 typo': get_1_deletion(get_1_insertion(get_1_replacement(query_3_terms))),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2371db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datatests\n",
    "import json\n",
    "\n",
    "with open('query_datatest.json', 'w') as fp:\n",
    "    json.dump(datatest, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "960395ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create author datatest\n",
    "author_1_terms = {}\n",
    "while len(author_1_terms) < numOfData:\n",
    "    keyword = random.choice(list(sname2dataset.keys()))\n",
    "    if keyword not in author_1_terms:\n",
    "        author_1_terms[keyword] = list(set(sname2dataset[keyword]))\n",
    "\n",
    "author_2_terms = {}\n",
    "while len(author_2_terms) < numOfData:\n",
    "    keyword = random.choice(list(author2dataset.keys()))\n",
    "    if keyword not in author_2_terms:\n",
    "        author_2_terms[keyword] = list(set(author2dataset[keyword]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "61a352dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest_author = {\n",
    "    1:{\n",
    "        '0 typo': author_1_terms,\n",
    "        '1 del': get_1_deletion(author_1_terms),\n",
    "        '1 insert': get_1_insertion(author_1_terms),\n",
    "        '1 replace': get_1_replacement(author_1_terms)\n",
    "    },\n",
    "    2:{\n",
    "        '0 typo': author_2_terms,\n",
    "        '1 del': get_1_deletion(author_2_terms),\n",
    "        '1 insert': get_1_insertion(author_2_terms),\n",
    "        '1 replace': get_1_replacement(author_2_terms),\n",
    "        'no space': removeSpace(author_2_terms)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7ec03596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save author datatests\n",
    "import json\n",
    "\n",
    "with open('author_datatest.json', 'w') as fp:\n",
    "    json.dump(datatest_author, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
